{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decission Trees\n",
    "\n",
    "Cuya traducción en español sería, árboles de decisión. \n",
    "\n",
    "Este algoritmo al igual que los SVM, puede realizar las tareas de regresión y clasificación con incluso múltiples salidas, también es parte fundamental del algoritmo Random Forest el cual es uno de los algoritmos más poderosos actualmente.\n",
    "\n",
    "Ahora veremos como entrenar, visualizar y realizar predicciones con los árboles de decisiones, llamaremos a las tareas de clasificación y regresión para los árboles como CART (CART : Clasificación and Regression Trees). Usaremos el algoritmo CART de entrenamiento dado por la librería scikit-learn, regularizaremos los árboles y los usaremos para tareas de regresión. Finalmente se verá también las debilidades del algoritmo Decission Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Setup\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"decision_trees\"\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(image_path(fig_id) + \".png\", format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y Visualización de un árbol de decisión\n",
    "\n",
    "Para entender los árboles de decisión se construirá uno y se mirará como se realiza una predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "X = iris.data[:, 2:] # largo y ancho del pétalo\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=\"/home/nicerova/Escritorio/iris_tree.dot\",\n",
    "    feature_names=iris.feature_names[2:],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](iris_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haciendo predicciones\n",
    "\n",
    "Veamos como es que trabaja un árbol de decisión:\n",
    "   \n",
    "Tenemos un nodo raiz en el cual tiene una \"pregunta\" y es que si el largo del pétalo es menor igual a 2.45 entonces en base a la respuesta de ésta pregunta es que podemos clasificar una flor, en este caso si la respuesta es SI entonces clasificado como setosa (la hoja izquierda).\n",
    "\n",
    "Si la respuesta es FALSE nos movemos hacia la derecha donde el nodo raiz tiene un nodo hijo que de nuevo nos hace una pregunta que esta vez es **¿el pétalo tiene ancho = 1.75?** formandose en base a la respuesta SI o NO (TRUE o FALSE) dos hojas (nodos sin hijos) las cuales clasificará el valor como versicolor o como virginica según sea la respuesta.\n",
    "\n",
    "Esto es realmente asi de simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tip:\n",
    "\n",
    "Uno de las muchas cualidades del algoritmo árbol de decisión es que requiere muy poca preparación de data, de hecho este algoritmo no requiere escalado de los datos o un centrado de estos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descripción del árbol\n",
    "\n",
    "En la imagen podemos ver la opción *samples* la cual es igual a la cantidad de valores de entrenamiento que se encuentran en esa clasificación: Podemos ver como en el nodo raiz se encuentran los 150 valores de entrenamiento inicial, luego en la hoja izquierda tenemos 50 plantas las cuales la longitud de su pétalo es menor o igual a 2.45cm y por el lado derecho tenemos los 100 valores restantes, luego de estos 100 en sus nodos hijos izquierdos y derechos tenemos 54 (que son la cantidad de plantas cuyo ancho de su pétalo es menor igual a 1.75cm) y 46 valores respectivamente.\n",
    "\n",
    "El atributo *value* en cada nodo indica cuantos de estos valores totales (vistos en el anterior párrafo) corresponden a cada clase: por ejemplo, tenemos en el nodo hijo más a la izquierda a 50 valores todos correspondientes a una única clase, la clase **setosa** por otro lado en el nodo color verde claro, 0 son **setosa**, 49 son **versicolor** y 5 son **virginica**.\n",
    "\n",
    "Finalmente, el atributo *gini* indica la cantidad de impureza de un nodo, un gini = 0 significa un nodo \"puro\" el cual todos los valores que ahí se ubican (valores de entrenamiento) son de la misma clase que indica la hoja. Por ejemplo podemos encontrar este nivel de pureza (Gini = 0) en la hoja que está más a la izquierda ya que los valores de entrenamiento todos son setosa y justamente esta hoja indica que todos ahi que todos los valores que caigan serán setosas. Veamos como es la ecuación con la cual calculamos el atributo *Gini* el cual es llamado aquí como puntaje gini (Gini score).\n",
    "\n",
    "$G_i = 1 - \\sum_{k=1}^n p_{i,k^2}$\n",
    "\n",
    "Donde:\n",
    "   * i es el i-ésimo nodo\n",
    "   * $p_{i,k}$ es la proporción de instancias de clase k entre las instancias de entrenamiento del i-ésimo nodo.\n",
    "   \n",
    "Por ejemplo, para la el hijo izquierdo del nodo derecho de la raiz tenemos :\n",
    "\n",
    "$1 - ( (0/54)^2  + (49/54)^2 + (5/54)^2 ) \\approx 0.168$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tip 2: \n",
    "\n",
    "Este algoritmo CART que tiene la librería scikit-learn solo produce árboles binarios, es decir, los nodos que no son hojas tienen siempre dos hijos (preguntas si o no) , sin embargo otros algoritmos como ID3 pueden producir árboles de decisión con nodos que tiene más de dos hijos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caja blanca, caja negra\n",
    "\n",
    "Como se vió es bastante simple la explicación de la clasificación que hizo nuestro árbol de decisión, incluso hasta se pudo graficar las preguntas que se hizo para clasificar nuestro valores, a este compartamiento se le llama modelos de caja blanca. Por otro lado, en el algoritmo RandomForest o en las redes Neuronales es muy complicado decir como es que se hizo las preguntas, como es que se tuvo la clasificación de cierta clase y todas estas preguntas que se podian responder en los modelos de caja blanca. Por ejemplo, si una red neuronal identifica una persona en una foto, no podemos nosotros explicar con certeza que parámetros de la foto influyó en la clasificación de la persona, pudo haber sido sus ojos, su boca, su rostro en general, esto no es del todo claro como en los modelos de caja blanca (hasta los podemos hacerlos manualmente !!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimación de probabilidades de las clases\n",
    "\n",
    "Un árbol de decisión también puede estimar la probabilidad de que una instancia pertenezca a una clase k particular: primero atraviesa el árbol para encontrar el nodo hoja para esta instancia, y luego devuelve la proporción de instancias de entrenamiento de la clase k en este nodo. \n",
    "\n",
    "Por ejemplo, supongamos que hemos encontrado una flor cuyos pétalos son de 5 cm de largo y 1,5 cm de ancho. El nodo hoja correspondiente es el nodo izquierdo de profundidad 2, por lo que el Árbol de decisión debería generar las siguientes probabilidades: 0% para Iris-Setosa (0/54), 90.7% para Iris-Versicolor (49/54) y 9.3% para Iris-Virginica (5/54). Y, por supuesto, si nos piden que se prediga la clase, se debería generar Iris-Versicolor (clase 1) ya que tiene la mayor probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]]) # las 3 clases con sus respectivas probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
